---
title: "Projects"
permalink: "/about/"
layout: page
---
Projects I've done 

#Smaller Projects

## Here are a series of brief explainers I made on the Transformer architecture: [Transformers](https://gabby-foxtrot-8e2.notion.site/Transformers-1066353a15494d8f82b677e226e777e0)

#AI
## [Project 1:  Transformer-From-Scratch ](https://github.com/bigtimecodersean/Transformer_From_Scratch)

This project was inspired by Andrej Karpathy's work at: https://www.youtube.com/watch?v=kCc8FmEb1nY

We will be building a decoder-only Transformer from scratch, and training it on a corpus of Wikipedia data, to try and generate Wikipedia-style text. We will be training on: Wikitext - V2. Wikitext - V2 is a 2M word subset of the Wikipedia corpus.

The goal for the project was to:
- Define a decoder transformer architecture
- Train on the WikiText dataset
- Generate infinite Wikipedia-like text

(Due to model complexity, of course, the generated text does not resemble Wikipedia-style English)




